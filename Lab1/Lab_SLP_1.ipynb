{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_SLP_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Xp_VZwjIhXPA",
        "ZIdGJ38HEmS7",
        "cfQsxre01R5e",
        "EJOlQ6FA14nP",
        "euGafi8mJJ-f",
        "nfyWwS6jJOhd",
        "N0xrkhI-JRaR",
        "7ioK4YelwpW4"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2Y0uIoYaPLC"
      },
      "source": [
        "#Εισαγωγικα\n",
        " Συνεργάτες :\n",
        " \n",
        "  Θεοδότη Στόικου 03117085\n",
        "\n",
        "  Δημήτρης Κουνούδης 03117169"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d5zj-oSOiZJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYCp5mHIvriK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f24600ab-9bf7-4181-cfea-c2a90332c662"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hY00fgBuCMe"
      },
      "source": [
        "!chmod +x install_openfst.sh\n",
        "!./install_openfst.sh &> /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjbkzLreyQgC"
      },
      "source": [
        "!pip install numpy\n",
        "!pip install nltk\n",
        "!pip install tqdm==4.48.2\n",
        "!pip install contractions==0.0.25\n",
        "!pip install gensim==3.8.3\n",
        "!pip install scikit-learn==0.23.2\n",
        "!pip install matplotlib==3.2.2\n",
        "!pip install plotly==4.10.0\n",
        "!sudo apt install python-pydot python-pydot-ng graphviz\n",
        "#cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xp_VZwjIhXPA"
      },
      "source": [
        "#Βημα 1\n",
        "1)a)Τα βήματα της προεπεξεργασίας του κειμένου λαμβάνουν χωρα στη συνάρτηση preprocess και συνεπώς και στις tokenize , clean text αφού έκαστη είναι εμφωλευμένη στην άλλη . Υπάρχει σχολιασμός στο κομμάτι του κώδικα πάνω στην ακριβή λειτουργία του καθενός. Αν θέλαμε να διατηρήσουμε τα σημεία στοίξης όπως την απόστροφο μπορούμε να αφαιρέσουμε τη συνάρτηση contractions.fix οπότε να διατηρήσουμε τη συμπτυγμένη μορφή των λέξεων και συνεπώς τις αποστρόφους .Επίσης θα μπορούσαμε να αφαιρέσουμε την εντολή   s = re.sub(r\"[^a-z\\s]\", \" \", s) που κρατάει μόνο τα lower case γράμματα ώστε να υιοθετήσουμε ενα λιγότερo \"επιθετικό\" preprocessing. Ετσι μπορούμε να διατηρήσουμε ένα πιο κατανοητό και αναλλοίωτο νόημα στο κείμενό μας.\n",
        "b)Προσθέτοντας περισσότερα βιβλία θα μπορούσαμε να περιλάβουμε διαφορετικά "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMTUEH4KYg5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fec61fb8-cc14-4571-d5f9-c5d745c77a97"
      },
      "source": [
        "import re\n",
        "import sys\n",
        "\n",
        "import contractions\n",
        "import nltk\n",
        "\n",
        "\n",
        "def download_corpus(corpus=\"gutenberg\"):\n",
        "    \"\"\"Download Project Gutenberg corpus, consisting of 18 classic books\n",
        "    Book list:\n",
        "       ['austen-emma.txt',\n",
        "        'austen-persuasion.txt',\n",
        "        'austen-sense.txt',\n",
        "        'bible-kjv.txt',\n",
        "        'blake-poems.txt',\n",
        "        'bryant-stories.txt',\n",
        "        'burgess-busterbrown.txt',\n",
        "        'carroll-alice.txt',\n",
        "        'chesterton-ball.txt',\n",
        "        'chesterton-brown.txt',\n",
        "        'chesterton-thursday.txt',\n",
        "        'edgeworth-parents.txt',\n",
        "        'melville-moby_dick.txt',\n",
        "        'milton-paradise.txt',\n",
        "        'shakespeare-caesar.txt',\n",
        "        'shakespeare-hamlet.txt',\n",
        "        'shakespeare-macbeth.txt',\n",
        "        'whitman-leaves.txt']\n",
        "    \"\"\"\n",
        "    nltk.download(corpus)\n",
        "    raw = nltk.corpus.__getattr__(corpus).raw()\n",
        "\n",
        "    return raw\n",
        "\n",
        "\n",
        "def identity_preprocess(s):\n",
        "    return s\n",
        "\n",
        "\n",
        "def clean_text(s):\n",
        "    s = s.strip()  # strip leading / trailing spaces\n",
        "    s = s.lower()  # convert to lowercase\n",
        "    s = contractions.fix(s)  # e.g. don't -> do not, you're -> you are\n",
        "    s = re.sub(\"\\s+\", \" \", s)  # strip multiple whitespace\n",
        "    s = re.sub(r\"[^a-z\\s]\", \" \", s)  # keep only lowercase letters and spaces\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def tokenize(s):\n",
        "    tokenized = [w for w in s.split(\" \") if len(w) > 0]  # Ignore empty string\n",
        "\n",
        "    return tokenized\n",
        "\n",
        "\n",
        "def preprocess(s):\n",
        "    return tokenize(clean_text(s))\n",
        "\n",
        "\n",
        "def process_file(corpus, preprocess=identity_preprocess):\n",
        "    lines = [preprocess(ln) for ln in corpus.split(\"\\n\")]\n",
        "    lines = [ln for ln in lines if len(ln) > 0]  # Ignore empty lines\n",
        "\n",
        "    return lines\n",
        "\n",
        "\n",
        "CORPUS = \"gutenberg\" \n",
        "raw_corpus = download_corpus(corpus=CORPUS)\n",
        "preprocessed = process_file(raw_corpus, preprocess=preprocess)\n",
        "\n",
        "#print(len(preprocessed))\n",
        "# print(type(preprocessed))\n",
        "#print(preprocessed[0:3])\n",
        "# for words in preprocessed:\n",
        "#     sys.stdout.write(\" \".join(words))\n",
        "#     sys.stdout.write(\"\\n\")\n",
        "# print(\"done\")\n",
        "# print(preprocessed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIdGJ38HEmS7"
      },
      "source": [
        "#Βημα 2\n",
        "  2) a)Δημιουργούμε το λεξικό που χρειάζεται τύπου dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b96hZIAFEdUq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "03fcbc34-39eb-4519-a6a4-3c70d571d916"
      },
      "source": [
        "lexicon = {}\n",
        "for line in preprocessed:\n",
        "  for word in line:\n",
        "    if word in lexicon:\n",
        "      lexicon[word] += 1 #if the word exist , increase by one its value\n",
        "    else:\n",
        "      lexicon[word] = 1 #if it doesn't exist initialize its value = 1\n",
        "# print(word)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-455795643f04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mlexicon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m#if the word exist , increase by one its value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'preprocessed' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-9MUZrSElTl"
      },
      "source": [
        "b)Εδώ φιλτράρουμε τις λέξεις που εμφανίζονται λιγότερο από πέντε φορές προσθέτοντας τις σε μια λίστα με τις λέξεις που πρέπει να διαγραφούν κι έπειτα τις διαγράφουμε.Αυτό το κάνουμε γιατι εφ'όσον το μοντέλο μας θα χρησιμοποιηθεί για στατιστικά στοιχεία δεν θέλουμε να κρατάμε λέξεις που εμφανίζονται πολυ λίγες φορές.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9stYWkbom_E"
      },
      "source": [
        "todelete = []\n",
        "for word in lexicon:\n",
        "  if lexicon[word] < 5:\n",
        "    todelete.append(word) #if its value is less than 5 add it to a list named todelete\n",
        "    # print(word)\n",
        "\n",
        "for word in todelete:\n",
        "    del lexicon[word] #delete the words that are added to the list todelete from the list lexicon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT5FIEcVFy7E"
      },
      "source": [
        "c)Παρακάτω ανοίγω το ζητούμενο αρχείο και αποθηκεύω μέσα το λεξικό με τη μορφή που μας ζητήθηκε"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp0hprPOo6Rn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30590b31-e020-4235-8fb4-58be4d606a33"
      },
      "source": [
        "!mkdir vocab\n",
        "\n",
        "vocab=open(\"vocab/words.vocab.txt\",\"w\")\n",
        "for word in lexicon:\n",
        "  vocab.write(word) #the word\n",
        "  vocab.write(\"\\t\") #the tab between them\n",
        "  vocab.write(str(lexicon[word])) #the value of the word\n",
        "  vocab.write(\"\\n\") #change line\n",
        "vocab.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘vocab’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfQsxre01R5e"
      },
      "source": [
        "# Βημα 3:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Z9yRhkC-Bb"
      },
      "source": [
        "3) a) Aντιστοιχίζουμε το γράμματα της αλφαβήτου με τα αντίστοιχα αυξανόμενα νούμερα αρχίζοντας από ε = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0TYH39QJxv6"
      },
      "source": [
        "def alphabet_indexing(alphabet):\n",
        "\tindexed_alphabet = []\n",
        "\tindexed_alphabet.append((\"<epsilon>\", 0)) #epsilon had value 0\n",
        "\ti = 1;\n",
        "\tfor letter in alphabet:\n",
        "\t\tindexed_alphabet.append((letter, str(i))) #each letter has increasing value starting from a=1\n",
        "\t\ti+=1\n",
        "\treturn indexed_alphabet\n",
        "\n",
        "def symbols_file(ab, filename = \"chars.syms\"):\n",
        "\tf = open(filename, \"w+\")\n",
        "\tfor pair in ab:\n",
        "\t\tf.write(pair[0] + \"\\t\\t\" + str(pair[1]) + \"\\n\") #pair[0] is the word itself and pair[1] is its value\n",
        "\t\n",
        "alphabet = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]\n",
        "symbols_file(alphabet_indexing(alphabet),\"vocab/chars.syms\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMZx0PaeDNN-"
      },
      "source": [
        "b)Εδω γίνεται ομοίως η αντιστοίχιση για τις λέξεις του lexicon με αύξοντες αριθμούς"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJRMxnJ_Tmfk"
      },
      "source": [
        "def lexicon_indexing(lexicon):\n",
        "\tindexed_lexicon = []\n",
        "\ti = 0;\n",
        "\tfor word in lexicon:\n",
        "\t\tindexed_lexicon.append((word, str(i)))\n",
        "\t\ti+=1\n",
        "\treturn indexed_lexicon\n",
        "\n",
        "symbols_file(lexicon_indexing(lexicon),\"vocab/words.syms\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJOlQ6FA14nP"
      },
      "source": [
        "# Βημα 4:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxWxgiyCEeeM"
      },
      "source": [
        "4)a)Υλοποιούμε τον μετατροπέα"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rqI4koTnhPw"
      },
      "source": [
        "def text_fst(Alphabet, filename = \"text.txt\"):\n",
        "    f = open(filename, \"w+\")\n",
        "    for pair in Alphabet:\n",
        "        for pair1 in Alphabet:\n",
        "            if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                continue \n",
        "            elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "            elif (pair[0]=='<epsilon>'):\n",
        "                f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "            elif (pair1[0]=='<epsilon>'):\n",
        "                f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "            else:\n",
        "                f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 1\"+ \"\\n\")\n",
        "    f.write(\"0\")\n",
        "    f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F947PUxzblqh"
      },
      "source": [
        "β, γ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOlivs_k9THN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ad845b8-3d66-4b5f-c672-63ec62130c53"
      },
      "source": [
        "!mkdir fsts\n",
        "text_fst(alphabet_indexing(alphabet),\"fsts/L.fst\")\n",
        "!fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/L.fst > fsts/L.binfst\n",
        "!fstinfo  fsts/L.binfst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘fsts’: File exists\n",
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       1\n",
            "# of arcs                                         728\n",
            "initial state                                     0\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               26\n",
            "# of output epsilons                              26\n",
            "input label multiplicity                          26.9643\n",
            "output label multiplicity                         26.9643\n",
            "# of accessible states                            1\n",
            "# of coaccessible states                          1\n",
            "# of connected states                             1\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     1\n",
            "input matcher                                     y\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    y\n",
            "output epsilons                                   y\n",
            "input label sorted                                y\n",
            "output label sorted                               n\n",
            "weighted                                          y\n",
            "cyclic                                            y\n",
            "cyclic at initial state                           y\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSxhFj4Jbr-y"
      },
      "source": [
        "ζ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvWdXW1rOo4P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73556c3e-a376-4a37-a110-20ff1458a82e"
      },
      "source": [
        "Alphabet = [('<epsilon>', 0), ('a', '1'), ('b', '2')]\n",
        "symbols_file(Alphabet, \"vocab/chars1.syms\")\n",
        "text_fst(Alphabet, \"fsts/Lsmall.fst\")\n",
        "\n",
        "!fstcompile -isymbols=vocab/chars1.syms -osymbols=vocab/chars1.syms fsts/Lsmall.fst > fsts/Lsmall.binfst\n",
        "!fstdraw --isymbols=vocab/chars1.syms --osymbols=vocab/chars1.syms -portrait fsts/Lsmall.binfst  | dot -Tjpg > fsts/Lsmall.jpg\n",
        "!fstinfo fsts/Lsmall.binfst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       1\n",
            "# of arcs                                         8\n",
            "initial state                                     0\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               2\n",
            "# of output epsilons                              2\n",
            "input label multiplicity                          2.75\n",
            "output label multiplicity                         2.75\n",
            "# of accessible states                            1\n",
            "# of coaccessible states                          1\n",
            "# of connected states                             1\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     1\n",
            "input matcher                                     y\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    y\n",
            "output epsilons                                   y\n",
            "input label sorted                                y\n",
            "output label sorted                               n\n",
            "weighted                                          y\n",
            "cyclic                                            y\n",
            "cyclic at initial state                           y\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euGafi8mJJ-f"
      },
      "source": [
        "# Βήμα 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA6IDDJSWIwb"
      },
      "source": [
        "5)α)Κατασκευάζω έναν αποδοχέα"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzbtGxa4i6mg"
      },
      "source": [
        "def Acceptor(Lexicon, Alphabet, filename = \"V.fst\"):\n",
        "    for i in range(0, len(Alphabet)):\n",
        "        temp = Alphabet[i]\n",
        "        Alphabet[i] = [temp, i]\n",
        "    f = open(filename, \"w+\")\n",
        "    i = 2\n",
        "    for word in Lexicon:\n",
        "        for j in range(0, len(word)):\n",
        "            if j==0:\n",
        "                f.write(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                # print(\"0\" + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\") \n",
        "                i += 1\n",
        "            elif (j== len(word) -1):\n",
        "                f.write( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                # print( str(i-1) + \" \" + \"1\" + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\") \n",
        "                i +=1\n",
        "            else:\n",
        "                f.write( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                # print( str(i-1) + \" \" + str(i) + \" \" + word[j] + \" \" + word[j] + \" 0\" + \"\\n\")\n",
        "                i+=1\n",
        "    f.write(\"1\")\n",
        "    # print(\"1\")\n",
        "    f.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WypNrYaTWRSK"
      },
      "source": [
        "β, γ, δ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaHbwrzUjRh3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3479819f-1923-4afc-bc6a-b533a83c3f7c"
      },
      "source": [
        "Acceptor(lexicon,alphabet_indexing(alphabet),\"fsts/V.fst\")\n",
        "!fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/V.fst > fsts/V.binfst\n",
        "!fstinfo fsts/V.binfst\n",
        "!fstdeterminize fsts/V.binfst  > fsts/V_determinized.binfst\n",
        "!fstminimize fsts/V_determinized.binfst > fsts/V_minimized.binfst\n",
        "!fstinfo fsts/V_minimized.binfst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       92745\n",
            "# of arcs                                         108186\n",
            "initial state                                     0\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          132.168\n",
            "output label multiplicity                         132.168\n",
            "# of accessible states                            92745\n",
            "# of coaccessible states                          92722\n",
            "# of connected states                             92722\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     92745\n",
            "input matcher                                     n\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          y\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                n\n",
            "output label sorted                               n\n",
            "weighted                                          n\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      n\n",
            "string                                            n\n",
            "weighted cycles                                   n\n",
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       9141\n",
            "# of arcs                                         18837\n",
            "initial state                                     0\n",
            "# of final states                                 1725\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          1\n",
            "output label multiplicity                         1\n",
            "# of accessible states                            9141\n",
            "# of coaccessible states                          9141\n",
            "# of connected states                             9141\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     9141\n",
            "input matcher                                     y\n",
            "output matcher                                    y\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          y\n",
            "input deterministic                               y\n",
            "output deterministic                              y\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                y\n",
            "output label sorted                               y\n",
            "weighted                                          n\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQ3G1wTMgcbO"
      },
      "source": [
        "ε)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eP2ng7YrkM_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58a71eef-9e10-4f78-b6a0-9ec13297a9aa"
      },
      "source": [
        "count = 0\n",
        "newlexicon = {}\n",
        "for i in lexicon:\n",
        "  if count == 10:\n",
        "    break\n",
        "  newlexicon[i] = lexicon[i]\n",
        "  count += 1\n",
        "Acceptor(newlexicon,alphabet_indexing(alphabet),\"fsts/Vsmall.fst\")\n",
        "!fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/Vsmall.fst > fsts/Vsmall.binfst\n",
        "!fstinfo fsts/Vsmall.binfst\n",
        "!fstdeterminize fsts/Vsmall.binfst  > fsts/Vsmall_determinized.binfst\n",
        "!fstminimize fsts/Vsmall_determinized.binfst > fsts/Vsmall_minimized.binfst\n",
        "!fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/Vsmall_minimized.binfst  | dot -Tjpg >fsts/Vsmall.jpg\n",
        "!fstinfo fsts/Vsmall_minimized.binfst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       43\n",
            "# of arcs                                         50\n",
            "initial state                                     0\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          1.04\n",
            "output label multiplicity                         1.04\n",
            "# of accessible states                            43\n",
            "# of coaccessible states                          42\n",
            "# of connected states                             42\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     43\n",
            "input matcher                                     n\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          y\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                n\n",
            "output label sorted                               n\n",
            "weighted                                          n\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      n\n",
            "string                                            n\n",
            "weighted cycles                                   n\n",
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       35\n",
            "# of arcs                                         42\n",
            "initial state                                     0\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          1\n",
            "output label multiplicity                         1\n",
            "# of accessible states                            35\n",
            "# of coaccessible states                          35\n",
            "# of connected states                             35\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     35\n",
            "input matcher                                     y\n",
            "output matcher                                    y\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          y\n",
            "input deterministic                               y\n",
            "output deterministic                              y\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                y\n",
            "output label sorted                               y\n",
            "weighted                                          n\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrVJXMD2lr2q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cec6359-da8b-4fb5-99e3-c81e46e5c24b"
      },
      "source": [
        "!fstcompose fsts/L.binfst fsts/V_minimized.binfst > fsts/Leven.binfst\n",
        "!fstinfo fsts/Leven.binfst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       9141\n",
            "# of arcs                                         746265\n",
            "initial state                                     0\n",
            "# of final states                                 1725\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               18837\n",
            "# of output epsilons                              237666\n",
            "input label multiplicity                          4.41912\n",
            "output label multiplicity                         26.6815\n",
            "# of accessible states                            9141\n",
            "# of coaccessible states                          9141\n",
            "# of connected states                             9141\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     9141\n",
            "input matcher                                     y\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    y\n",
            "output epsilons                                   y\n",
            "input label sorted                                y\n",
            "output label sorted                               n\n",
            "weighted                                          y\n",
            "cyclic                                            y\n",
            "cyclic at initial state                           y\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   y\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfyWwS6jJOhd"
      },
      "source": [
        "# Βήμα 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4PlazTcgCqz"
      },
      "source": [
        "6.α, β)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT3PSZ2pnHdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c45f828-5bd7-4237-e2b4-dd4a993d298d"
      },
      "source": [
        "Lexicon_cit = [\"cit\"]\n",
        "Acceptor(Lexicon_cit,alphabet_indexing(alphabet), \"fsts/cit.fst\")\n",
        "! fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/cit.fst > fsts/cit.binfst\n",
        "! fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/cit.binfst  | dot -Tjpg >fsts/cit.jpg\n",
        "! fstcompose fsts/cit.binfst fsts/Leven.binfst  > fsts/min_distance1.binfst\n",
        "! fstinfo fsts/min_distance1.binfst\n",
        "!fstshortestpath fsts/min_distance1.binfst | fstrmepsilon > fsts/out1.binfst\n",
        "!fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/out1.binfst  | dot -Tjpg >fsts/out1.jpg\n",
        "!fstinfo fsts/out1.binfst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       36564\n",
            "# of arcs                                         159282\n",
            "initial state                                     0\n",
            "# of final states                                 1725\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               75348\n",
            "# of output epsilons                              27423\n",
            "input label multiplicity                          4.26835\n",
            "output label multiplicity                         1.70957\n",
            "# of accessible states                            36564\n",
            "# of coaccessible states                          36564\n",
            "# of connected states                             36564\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     36564\n",
            "input matcher                                     y\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    y\n",
            "output epsilons                                   y\n",
            "input label sorted                                y\n",
            "output label sorted                               n\n",
            "weighted                                          y\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n",
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       4\n",
            "# of arcs                                         3\n",
            "initial state                                     3\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          1\n",
            "output label multiplicity                         1\n",
            "# of accessible states                            4\n",
            "# of coaccessible states                          4\n",
            "# of connected states                             4\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     4\n",
            "input matcher                                     y\n",
            "output matcher                                    y\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               y\n",
            "output deterministic                              y\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                y\n",
            "output label sorted                               y\n",
            "weighted                                          y\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJr6yikkn8BV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e935042-ddcc-4d9e-e419-e9c016c9ad7c"
      },
      "source": [
        "Lexicon_cwt = [\"cwt\"]\n",
        "Acceptor(Lexicon_cwt,alphabet_indexing(alphabet), \"fsts/cwt.fst\")\n",
        "! fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/cwt.fst > fsts/cwt.binfst\n",
        "! fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/cwt.binfst  | dot -Tjpg >fsts/cwt.jpg\n",
        "! fstcompose fsts/cwt.binfst fsts/Leven.binfst  > fsts/min_distance2.binfst\n",
        "! fstinfo fsts/min_distance2.binfst\n",
        "!fstshortestpath fsts/min_distance2.binfst | fstrmepsilon > fsts/out2.binfst\n",
        "!fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/out2.binfst  | dot -Tjpg >fsts/out2.jpg\n",
        "!fstinfo fsts/out2.binfst"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       36564\n",
            "# of arcs                                         159282\n",
            "initial state                                     0\n",
            "# of final states                                 1725\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               75348\n",
            "# of output epsilons                              27423\n",
            "input label multiplicity                          4.26835\n",
            "output label multiplicity                         1.70957\n",
            "# of accessible states                            36564\n",
            "# of coaccessible states                          36564\n",
            "# of connected states                             36564\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     36564\n",
            "input matcher                                     y\n",
            "output matcher                                    n\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               n\n",
            "output deterministic                              n\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    y\n",
            "output epsilons                                   y\n",
            "input label sorted                                y\n",
            "output label sorted                               n\n",
            "weighted                                          y\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n",
            "fst type                                          vector\n",
            "arc type                                          standard\n",
            "input symbol table                                none\n",
            "output symbol table                               none\n",
            "# of states                                       4\n",
            "# of arcs                                         3\n",
            "initial state                                     3\n",
            "# of final states                                 1\n",
            "# of input/output epsilons                        0\n",
            "# of input epsilons                               0\n",
            "# of output epsilons                              0\n",
            "input label multiplicity                          1\n",
            "output label multiplicity                         1\n",
            "# of accessible states                            4\n",
            "# of coaccessible states                          4\n",
            "# of connected states                             4\n",
            "# of connected components                         1\n",
            "# of strongly conn components                     4\n",
            "input matcher                                     y\n",
            "output matcher                                    y\n",
            "input lookahead                                   n\n",
            "output lookahead                                  n\n",
            "expanded                                          y\n",
            "mutable                                           y\n",
            "error                                             n\n",
            "acceptor                                          n\n",
            "input deterministic                               y\n",
            "output deterministic                              y\n",
            "input/output epsilons                             n\n",
            "input epsilons                                    n\n",
            "output epsilons                                   n\n",
            "input label sorted                                y\n",
            "output label sorted                               y\n",
            "weighted                                          y\n",
            "cyclic                                            n\n",
            "cyclic at initial state                           n\n",
            "top sorted                                        n\n",
            "accessible                                        y\n",
            "coaccessible                                      y\n",
            "string                                            n\n",
            "weighted cycles                                   n\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0xrkhI-JRaR"
      },
      "source": [
        "# Βήμα 7\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIsS0Dv5Zev8"
      },
      "source": [
        "7)a)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdEvdXJxok6v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20c44057-4f2c-4e64-86ed-c6844e601974"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "def download(url, file):\n",
        "    if not os.path.isfile(file):\n",
        "        print(\"Download file... \" + file + \" ...\")\n",
        "        urlretrieve(url,file)\n",
        "        print(\"File downloaded\")\n",
        "\n",
        "download(\"https://raw.githubusercontent.com/slp-ntua/slp-labs/master/lab1/data/spell_test.txt\",\"evaluation_data.txt\")\n",
        "print(\"All the files are downloaded\")#If the downloaded file is a zip file than you can use below function to unzip it."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All the files are downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jHJ26lWJZijB"
      },
      "source": [
        "b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqd6CzR3_bYK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3b865d2-ba82-4e5b-e810-262d26b5ba29"
      },
      "source": [
        "!ls vocab/\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "chars1.syms  chars.syms  words.syms  words.vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQKgEJgHrLyR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb2cb26-8b81-4422-b071-b23cab2a88c9"
      },
      "source": [
        "import random\n",
        "lines = []\n",
        "with open(\"evaluation_data.txt\") as file:\n",
        "    for line in file:\n",
        "        lines.append(line)\n",
        "text = []\n",
        "dictionary = {}\n",
        "for line in lines:\n",
        "    text = []\n",
        "    text.append(tokenize(clean_text(line)))\n",
        "    for i in range (1,len(text[0])):\n",
        "      dictionary.update({text[0][i]:text[0][0]})\n",
        "for counter in range(0,20):\n",
        "  key = random.choice(list(dictionary.keys()))\n",
        "  value = dictionary[key]\n",
        "  Lexicon_test = [key]\n",
        "  f = open('new.txt', 'w+')\n",
        "  f.truncate(0)\n",
        "  f.close()\n",
        "  fileopen = open(\"new.txt\",\"w+\")\n",
        "  for i in range(0,len(key)):\n",
        "    fileopen.write(\"%s \" %str(i))\n",
        "    fileopen.write(\" %s \" %str(i+1))\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\"0\\n\")\n",
        "  fileopen.write(str(len(key)))\n",
        "  fileopen.close()\n",
        "  Acceptor(Lexicon_test, alphabet_indexing(alphabet), \"new.txt\")\n",
        "  !fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms new.txt > fsts/test.binfst\n",
        "  !fstcompose fsts/test.binfst fsts/Leven.binfst  > fsts/min_distance.binfst\n",
        "  print(\"Wrong word = \" ,key,\", actual word = \",value)\n",
        "  print(\"What we got from the transducer =\", end =\" \")  \n",
        "  !fstshortestpath fsts/min_distance.binfst | fstrmepsilon | fsttopsort | fstprint -osymbols=vocab/chars.syms | cut -f4 | grep -v \"<epsilon>\" | head -n -1 | tr -d '\\n'\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong word =  rember , actual word =  remember\n",
            "What we got from the transducer = member\n",
            "Wrong word =  sorces , actual word =  sources\n",
            "What we got from the transducer = forces\n",
            "Wrong word =  chaphter , actual word =  chapter\n",
            "What we got from the transducer = chapter\n",
            "Wrong word =  viseted , actual word =  visited\n",
            "What we got from the transducer = visited\n",
            "Wrong word =  buiscits , actual word =  biscuits\n",
            "What we got from the transducer = biscuits\n",
            "Wrong word =  dessicate , actual word =  desiccate\n",
            "What we got from the transducer = delicate\n",
            "Wrong word =  compair , actual word =  compare\n",
            "What we got from the transducer = complain\n",
            "Wrong word =  inconvienient , actual word =  inconvenient\n",
            "What we got from the transducer = inconvenient\n",
            "Wrong word =  proble , actual word =  problem\n",
            "What we got from the transducer = problem\n",
            "Wrong word =  somone , actual word =  someone\n",
            "What we got from the transducer = soone\n",
            "Wrong word =  bisquits , actual word =  biscuits\n",
            "What we got from the transducer = biscuits\n",
            "Wrong word =  curtians , actual word =  curtains\n",
            "What we got from the transducer = curtains\n",
            "Wrong word =  cak , actual word =  cake\n",
            "What we got from the transducer = oak\n",
            "Wrong word =  scarcly , actual word =  scarcely\n",
            "What we got from the transducer = scarcely\n",
            "Wrong word =  stomac , actual word =  stomach\n",
            "What we got from the transducer = stomach\n",
            "Wrong word =  magnifiscant , actual word =  magnificent\n",
            "What we got from the transducer = magnificent\n",
            "Wrong word =  opisite , actual word =  opposite\n",
            "What we got from the transducer = opposite\n",
            "Wrong word =  inistals , actual word =  initials\n",
            "What we got from the transducer = instant\n",
            "Wrong word =  biscutes , actual word =  biscuits\n",
            "What we got from the transducer = disputes\n",
            "Wrong word =  inistals , actual word =  initials\n",
            "What we got from the transducer = instant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjFCKp8RpR_"
      },
      "source": [
        "#Βημα 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Qq4URSuWTn"
      },
      "source": [
        "Tρέχοντας τις κατάλληλες εντολές στο Bash δημιουργούμε απο έναν αποδοχέα τόσο για τη σωστή όσο και για τη λάθος λέξη και εφ όσον η συνάρτηση μας Acceptor δέχεται στην είσοδο dictionary , εισάγουμε τις λέξεις ως τέτοια. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIdd39swRq_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1a27f66-705a-4122-e8d1-64637faf19b7"
      },
      "source": [
        "!mkdir nameit\n",
        "\n",
        "LexiconAbandonned = {}\n",
        "LexiconAbandonned[\"abandonned\"] = \"\"\n",
        "Acceptor(LexiconAbandonned,Alphabet,\"fsts/Abandonned.txt\")\n",
        "LexiconAbandoned = {}\n",
        "LexiconAbandoned[\"abandoned\"] = \"\"\n",
        "Acceptor(LexiconAbandoned,Alphabet,\"fsts/Abandoned.txt\")\n",
        "\n",
        "!fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/Abandonned.txt > fsts/Abandonned.binfst\n",
        "!fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/Abandoned.txt > fsts/Abandoned.binfst\n",
        "\n",
        "!fstcompose fsts/Abandonned.binfst fsts/L.binfst > fsts/produced.binfst\n",
        "!fstcompose fsts/produced.binfst fsts/Abandoned.binfst > fsts/MLN.fst\n",
        "\n",
        "f = open(\"temporary.txt\",mode = \"w+\")\n",
        "f.close()\n",
        "!fstshortestpath fsts/MLN.fst | fstprint --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms --show_weight_one > temporary.txt\n",
        "with open(\"temporary.txt\", \"r\") as a_file:\n",
        "  content = a_file.readlines()\n",
        "  for line in content:\n",
        "    line_list = line.rstrip().split(sep = '\\t') \n",
        "    print(line_list)\n",
        "    # "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘nameit’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dT__U___5enF"
      },
      "source": [
        "δ)ε)Ουσιαστικά εδώ ανοίγω το αρχείο wiki αποθηκεύω τις γραμμές του σε μια μεταβλητή ( wiki_content) την οποία έπειτα τρέχω γραμμή γραμμή. Για κάθε στοιχείο της γραμμής (2 αφού υπάρχει μία η σωστή και μία η λάθος λέξη) δημιουργώ από ένα αυτόματο και φτιάχνω τη σύνθεση που κάνω και στα ερωτήματα α,β,γ της οποίας τον πίνακα μεταβάσεων αποθηκεύω κάθε φορά στο αρχείο temporary.Κάθε φορά τρέχω το αρχείο αυτό και κρατάω σε μια λίστα μόνο τις γραμμές του που αποτελούνται απο > 2 στοιχεία (ώστε να μην είναι τελική κατάσταση) και έχουν  line_list[4] != \"0\" (κόστος > 0) δηλαδή τις μεταβάσεις που διορθώνουν λάθος και στο αντίστοιχο key του dictionary τοποθετώ τον αριθμό των εμφανίσεων . Έτσι έχω και τα δύο ερωτήματα στο ίδιο dictionary .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKMDOeTf5ZvD"
      },
      "source": [
        "# SE SXOLIA GIATI PAIRNEI POLU WRA NA TREKSEI\n",
        "\n",
        "# temporary_dictionary = {}\n",
        "# with open(\"wiki.txt\") as wiki: \n",
        "#     wiki_content=wiki.readlines()\n",
        "#     for wiki_line in wiki_content:\n",
        "#         wiki_list = wiki_line.rstrip().split(sep = '\\t')\n",
        "#         LexiconAbandonned = {}\n",
        "#         LexiconAbandonned[wiki_list[0]] = \"\"\n",
        "#         Acceptor(LexiconAbandonned,Alphabet,\"fsts/Abandonned.txt\")\n",
        "#         LexiconAbandoned = {}\n",
        "#         LexiconAbandoned[wiki_list[1]] = \"\"\n",
        "#         Acceptor(LexiconAbandoned,Alphabet,\"fsts/Abandoned.txt\")\n",
        "#         !fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/Abandonned.txt > fsts/Abandonned.binfst\n",
        "#         !fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/Abandoned.txt > fsts/Abandoned.binfst\n",
        "#         !fstcompose fsts/Abandonned.binfst fsts/L.binfst > fsts/produced.binfst\n",
        "#         !fstcompose fsts/produced.binfst fsts/Abandoned.binfst > fsts/MLN.fst\n",
        "#         f = open(\"temporary.txt\",mode = \"w+\")\n",
        "#         f.close()\n",
        "#         !fstshortestpath fsts/MLN.fst | fstprint --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms --show_weight_one > temporary.txt\n",
        "#         with open(\"temporary.txt\", \"r\") as a_file:\n",
        "#             content = a_file.readlines()\n",
        "#             for line in content:\n",
        "#                 line_list = line.rstrip().split(sep = '\\t') \n",
        "#                 if len(line_list) > 2 and line_list[4] != \"0\":\n",
        "#                     if (line_list[2],line_list[3]) not in temporary_dictionary:\n",
        "#                         temporary_dictionary[(line_list[2],line_list[3])] = int(line_list[4])\n",
        "#                     else:\n",
        "#                         temporary_dictionary[(line_list[2],line_list[3])] += int(line_list[4])\n",
        "# print(temporary_dictionary)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEeoegvDzH17"
      },
      "source": [
        "f = open(\"edits.txt\",\"w+\")\n",
        "for key, value in temporary_dictionary.items():\n",
        "    f.write(str(key[0]) + \"\\t\" + str(key[1]) + \"\\t\" + str(value) + \"\\r\")\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehzJ7Vh0b1z3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ada45a98-6126-4a39-8b66-170fcc1ad8ae"
      },
      "source": [
        "import numpy as np\n",
        "f = open(\"edits.txt\",\"r\")\n",
        "temporary_dictionary = {}\n",
        "content = f.readlines()\n",
        "for line in content:\n",
        "    line_list = line.rstrip().split(sep = '\\t')\n",
        "    temporary_dictionary[(line_list[0],line_list[1])] = int(line_list[2])\n",
        "sum = 0\n",
        "for x in temporary_dictionary:\n",
        "    sum += int(temporary_dictionary[x])\n",
        "for x in temporary_dictionary:\n",
        "    temporary_dictionary[x] = - np.log2(temporary_dictionary[x] / sum)\n",
        "\n",
        "print(temporary_dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{('n', '<epsilon>'): 6.149747119504682, ('<epsilon>', 'r'): 6.149747119504682, ('y', 'i'): 6.149747119504682, ('<epsilon>', 'i'): 4.564784618783526, ('o', 'a'): 4.149747119504682, ('b', '<epsilon>'): 6.149747119504682, ('u', 't'): 6.149747119504682, ('t', 'u'): 6.149747119504682, ('c', '<epsilon>'): 4.149747119504682, ('e', 'i'): 6.149747119504682, ('<epsilon>', 's'): 6.149747119504682, ('a', 'e'): 3.8278190246173196, ('i', 'a'): 4.564784618783526, ('<epsilon>', 'b'): 4.564784618783526, ('r', 'b'): 6.149747119504682, ('b', 'r'): 6.149747119504682, ('s', 'c'): 6.149747119504682, ('<epsilon>', 'e'): 5.149747119504682, ('s', 't'): 6.149747119504682, ('b', 'p'): 5.149747119504682, ('<epsilon>', 'n'): 6.149747119504682, ('i', 'c'): 6.149747119504682, ('c', 'n'): 6.149747119504682, ('i', '<epsilon>'): 6.149747119504682, ('u', 'a'): 6.149747119504682, ('t', '<epsilon>'): 6.149747119504682, ('l', '<epsilon>'): 6.149747119504682, ('n', 's'): 6.149747119504682, ('c', 's'): 6.149747119504682, ('e', 'a'): 6.149747119504682, ('a', 'i'): 6.149747119504682, ('<epsilon>', 'c'): 6.149747119504682, ('<epsilon>', 'l'): 5.149747119504682, ('<epsilon>', 'a'): 6.149747119504682, ('a', 'o'): 3.5647846187835257, ('<epsilon>', 'm'): 2.690315500867385, ('<epsilon>', 'o'): 6.149747119504682}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JlrJl3G5Cb0"
      },
      "source": [
        "στ)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHf0ahfBb5Ay"
      },
      "source": [
        "def edits_text_fst(Alphabet, temporary_dictionary, filename = \"edits_text.txt\"):\n",
        "    f = open(filename, \"w+\")\n",
        "    for pair in Alphabet:\n",
        "        for pair1 in Alphabet:\n",
        "            if  (pair[0] == pair1[0] ==\"<epsilon>\"):\n",
        "                continue \n",
        "            elif (pair[0] == pair1[0] and pair[0]!=\"<epsilon>\"):\n",
        "                f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" 0\" + \"\\n\")\n",
        "            elif (pair[0]=='<epsilon>'):\n",
        "                if (pair[0],pair1[0]) not in temporary_dictionary:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(1000)+ \"\\n\")\n",
        "                else:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(temporary_dictionary[(pair[0],pair1[0])])+ \"\\n\")\n",
        "            elif (pair1[0]=='<epsilon>'):\n",
        "                if (pair[0],pair1[0]) not in temporary_dictionary:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(1000)+ \"\\n\")\n",
        "                else:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(temporary_dictionary[(pair[0],pair1[0])])+ \"\\n\")            \n",
        "            else:\n",
        "                if (pair[0],pair1[0]) not in temporary_dictionary:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(1000)+ \"\\n\")\n",
        "                else:\n",
        "                    f.write(\"0\" + \" 0 \" + pair[0] + \" \" + pair1[0] + \" \" + str(temporary_dictionary[(pair[0],pair1[0])])+ \"\\n\")    \n",
        "    f.write(\"0\")\n",
        "    f.close()\n",
        "\n",
        "edits_text_fst(alphabet_indexing(alphabet),temporary_dictionary,\"fsts/edit_text.txt\")\n",
        "!fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/edit_text.txt > fsts/E.binfst\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsoiJLbX5FIS"
      },
      "source": [
        "ζ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JduscVS6b9SP"
      },
      "source": [
        "!fstcompose fsts/E.binfst fsts/V_minimized.binfst > fsts/New_Leven.binfst\n",
        "# !fstinfo fsts/New_Leven.binfst\n",
        "Lexicon_cit = [\"cit\"]\n",
        "Acceptor(Lexicon_cit,alphabet_indexing(alphabet), \"fsts/cit.fst\")\n",
        "! fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/cit.fst > fsts/cit.binfst\n",
        "# ! fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/cit.binfst  | dot -Tjpg >fsts/cit.jpg\n",
        "! fstcompose fsts/cit.binfst fsts/New_Leven.binfst  > fsts/min_distance1.binfst\n",
        "# ! fstinfo fsts/min_distance1.binfst\n",
        "!fstshortestpath fsts/min_distance1.binfst | fstrmepsilon > fsts/out3.binfst\n",
        "!fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/out3.binfst  | dot -Tjpg >fsts/out3.jpg\n",
        "# !fstinfo fsts/out3.binfst\n",
        "\n",
        "Lexicon_cwt = [\"cwt\"]\n",
        "Acceptor(Lexicon_cwt,alphabet_indexing(alphabet), \"fsts/cwt.fst\")\n",
        "! fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/cwt.fst > fsts/cwt.binfst\n",
        "! fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/cwt.binfst  | dot -Tjpg >fsts/cwt.jpg\n",
        "! fstcompose fsts/cwt.binfst fsts/New_Leven.binfst  > fsts/min_distance2.binfst\n",
        "# ! fstinfo fsts/min_distance2.binfst\n",
        "!fstshortestpath fsts/min_distance2.binfst | fstrmepsilon > fsts/out4.binfst\n",
        "!fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/out4.binfst  | dot -Tjpg >fsts/out4.jpg\n",
        "# !fstinfo fsts/out4.binfst\n",
        "\n",
        "import random\n",
        "lines = []\n",
        "with open(\"evaluation_data.txt\") as file:\n",
        "    for line in file:\n",
        "        lines.append(line)\n",
        "text = []\n",
        "dictionary = {}\n",
        "for line in lines:\n",
        "    text = []\n",
        "    text.append(tokenize(clean_text(line)))\n",
        "    for i in range (1,len(text[0])):\n",
        "      dictionary.update({text[0][i]:text[0][0]})\n",
        "for counter in range(0,20):\n",
        "  key = random.choice(list(dictionary.keys()))\n",
        "  value = dictionary[key]\n",
        "  Lexicon_test = [key]\n",
        "  f = open('new.txt', 'w+')\n",
        "  f.truncate(0)\n",
        "  f.close()\n",
        "  fileopen = open(\"new.txt\",\"w+\")\n",
        "  for i in range(0,len(key)):\n",
        "    fileopen.write(\"%s \" %str(i))\n",
        "    fileopen.write(\" %s \" %str(i+1))\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\"0\\n\")\n",
        "  fileopen.write(str(len(key)))\n",
        "  fileopen.close()\n",
        "  Acceptor(Lexicon_test, alphabet_indexing(alphabet), \"new.txt\")\n",
        "  !fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms new.txt > test.binfst\n",
        "  !fstcompose test.binfst fsts/New_Leven.binfst  > min_distance.binfst\n",
        "  print(\"Wrong word = \" ,key,\", actual word = \",value)\n",
        "  print(\"What we got from the transducer =\", end =\" \")  \n",
        "  !fstshortestpath min_distance.binfst | fstrmepsilon | fsttopsort | fstprint -osymbols=vocab/chars.syms | cut -f4 | grep -v \"<epsilon>\" | head -n -1 | tr -d '\\n'\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVdr5siybG30"
      },
      "source": [
        "# Βημα 9\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGvedkXa5PFd"
      },
      "source": [
        "α)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1BpTOwxbKdo"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "lexicon = {}\n",
        "total_words = 0\n",
        "for line in preprocessed:\n",
        "  for word in line:\n",
        "    if word in lexicon:\n",
        "      lexicon[word] += 1 #if the word exist , increase by one its value\n",
        "    else:\n",
        "      lexicon[word] = 1 #if it doesn't exist initialize its value = 1\n",
        "# print(word)\n",
        "todelete = []\n",
        "for word in lexicon:\n",
        "  if lexicon[word] < 5:\n",
        "    todelete.append(word) #if its value is less than 5 add it to a list named todelete\n",
        "    # print(word)\n",
        "\n",
        "for word in todelete:\n",
        "    del lexicon[word] #delete the words that are added to the list todelete from the list lexicon\n",
        "\n",
        "for word in lexicon:\n",
        "    total_words += lexicon[word]\n",
        "\n",
        "for word in lexicon:\n",
        "    lexicon[word] = - np.log2(lexicon[word] / total_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFL396Uk5P-t"
      },
      "source": [
        "β,γ,δ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mzzaap2bbMUg"
      },
      "source": [
        "def W_fst(Lexicon,Alphabet,filename = \"fsts/W.fst\"):\n",
        "    f = open(filename, \"w+\")\n",
        "    for word in Lexicon:\n",
        "        f.write( \"0\" + \" \" + \"0\" + \" \" + word + \" \" + word + \" \" + str(lexicon[word]) + \"\\n\")\n",
        "    f.write(\"0\")\n",
        "    f.close()\n",
        "\n",
        "\n",
        "# mylexicon = {}\n",
        "# mylexicon[\"this\"] = 3\n",
        "# mylexicon[\"that\"] = 5\n",
        "W_fst(lexicon[0:3],Alphabet,)\n",
        "!fstcompile -isymbols=vocab/words.syms -osymbols=vocab/words.syms fsts/W.fst > fsts/W.binfst\n",
        "\n",
        "!fstcompose fsts/Leven.binfst fsts/W.binfst > fsts/LVW.binfst\n",
        "!fstcompose fsts/New_Leven.binfst fsts/W.binfst > fsts/EVW.binfst\n",
        "\n",
        "# !fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait  fsts/EVW.binfst  | dot -Tjpg > fsts/EVW.jpg\n",
        "# !fstshortestpath fsts/EVW.binfst | fstrmepsilon > fsts/out7.binfst\n",
        "# !fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/out7.binfst  | dot -Tjpg >fsts/out7.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOLkSCEH5WVq"
      },
      "source": [
        "ε,ζ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eepiWsvmbg4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0d4bd91-24b5-4b5b-cc8f-be4169bca0ba"
      },
      "source": [
        "\n",
        "Lexicon_cit = [\"cit\"]\n",
        "Acceptor(Lexicon_cit,alphabet_indexing(alphabet), \"fsts/cit.fst\")\n",
        "! fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/cit.fst > fsts/cit.binfst\n",
        "# ! fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/cit.binfst  | dot -Tjpg >fsts/cit.jpg\n",
        "! fstcompose fsts/cit.binfst fsts/LVW.binfst  > fsts/min_distance1.binfst\n",
        "# ! fstinfo fsts/min_distance1.binfst\n",
        "!fstshortestpath fsts/min_distance1.binfst | fstrmepsilon > fsts/out5.binfst\n",
        "!fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/out5.binfst  | dot -Tjpg >fsts/out5.jpg\n",
        "# !fstinfo fsts/out3.binfst\n",
        "\n",
        "Lexicon_cwt = [\"cwt\"]\n",
        "Acceptor(Lexicon_cwt,alphabet_indexing(alphabet), \"fsts/cwt.fst\")\n",
        "! fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms fsts/cwt.fst > fsts/cwt.binfst\n",
        "! fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/cwt.binfst  | dot -Tjpg >fsts/cwt.jpg\n",
        "! fstcompose fsts/cwt.binfst fsts/LVW.binfst  > fsts/min_distance2.binfst\n",
        "# ! fstinfo fsts/min_distance2.binfst\n",
        "!fstshortestpath fsts/min_distance2.binfst | fstrmepsilon > fsts/out6.binfst\n",
        "!fstdraw --isymbols=vocab/chars.syms --osymbols=vocab/chars.syms -portrait fsts/out6.binfst  | dot -Tjpg >fsts/out6.jpg\n",
        "# !fstinfo fsts/out4.binfst\n",
        "\n",
        "import random\n",
        "lines = []\n",
        "with open(\"evaluation_data.txt\") as file:\n",
        "    for line in file:\n",
        "        lines.append(line)\n",
        "text = []\n",
        "dictionary = {}\n",
        "for line in lines:\n",
        "    text = []\n",
        "    text.append(tokenize(clean_text(line)))\n",
        "    for i in range (1,len(text[0])):\n",
        "      dictionary.update({text[0][i]:text[0][0]})\n",
        "for counter in range(0,5):\n",
        "  key = random.choice(list(dictionary.keys()))\n",
        "  value = dictionary[key]\n",
        "  Lexicon_test = [key]\n",
        "  f = open('new.txt', 'w+')\n",
        "  f.truncate(0)\n",
        "  f.close()\n",
        "  fileopen = open(\"new.txt\",\"w+\")\n",
        "  for i in range(0,len(key)):\n",
        "    fileopen.write(\"%s \" %str(i))\n",
        "    fileopen.write(\" %s \" %str(i+1))\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\" %s \" %key[i])\n",
        "    fileopen.write(\"0\\n\")\n",
        "  fileopen.write(str(len(key)))\n",
        "  fileopen.close()\n",
        "  Acceptor(Lexicon_test, alphabet_indexing(alphabet), \"new.txt\")\n",
        "  !fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms new.txt > test.binfst\n",
        "  !fstcompose test.binfst fsts/LVW.binfst  > min_distance.binfst\n",
        "  print(\"Wrong word = \" ,key,\", actual word = \",value)\n",
        "  print(\"What we got from the transducer =\", end =\" \")  \n",
        "  !fstshortestpath min_distance.binfst | fstrmepsilon | fsttopsort | fstprint -osymbols=vocab/chars.syms | cut -f4 | grep -v \"<epsilon>\" | head -n -1 | tr -d '\\n'\n",
        "  print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wrong word =  initals , actual word =  initials\n",
            "What we got from the transducer = ii\n",
            "Wrong word =  muinets , actual word =  minutes\n",
            "What we got from the transducer = vi\n",
            "Wrong word =  ment , actual word =  meant\n",
            "What we got from the transducer = vi\n",
            "Wrong word =  poartry , actual word =  poetry\n",
            "What we got from the transducer = vi\n",
            "Wrong word =  occurence , actual word =  occurrence\n",
            "What we got from the transducer = ur\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmPYpIXbqsIo"
      },
      "source": [
        "# Βημα 10\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9nTIbexquZH"
      },
      "source": [
        "def Leven_accuracy(fst,filename):\n",
        "    with open(filename,\"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for line in lines:\n",
        "            line = line.replace(\":\",\"\")\n",
        "            line = line.split()\n",
        "            for i in range(len(line) - 1):\n",
        "                f = open('new.txt', 'w+')\n",
        "                f.truncate(0)\n",
        "                f.close()\n",
        "                fileopen = open(\"new.txt\",\"w+\")\n",
        "                for j in range(len(line[i+1])):\n",
        "                    fileopen.write(\"%s \" %str(j))\n",
        "                    fileopen.write(\" %s \" %str(j+1))\n",
        "                    fileopen.write(\" %s \" %line[i+1][j])\n",
        "                    fileopen.write(\" %s \" %line[i+1][j])\n",
        "                    fileopen.write(\"0\\n\")\n",
        "                fileopen.write(str(len(line[i+1])))\n",
        "\n",
        "                fileopen.close()\n",
        "                # break\n",
        "                Lexicon_test = {}\n",
        "                Lexicon_test[line[i+1]] = line[i+1]\n",
        "                Acceptor(Lexicon_test, alphabet_indexing(alphabet), \"new.txt\")\n",
        "                !fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms new.txt > test.binfst\n",
        "                !fstcompose test.binfst fsts/Leven.binfst  > min_distance.binfst\n",
        "                f = open('new.txt', 'w+')\n",
        "                f.truncate(0)\n",
        "                f.close()\n",
        "                \n",
        "                !fstshortestpath min_distance.binfst | fstrmepsilon | fsttopsort | fstprint -osymbols=vocab/chars.syms | cut -f4 | grep -v \"<epsilon>\" | head -n -1 | tr -d '\\n' > new.txt\n",
        "                fileopen = open(\"new.txt\",\"r\")\n",
        "                for content in fileopen.readlines():\n",
        "                    content = content.rstrip().split(sep = '\\t')\n",
        "                    if content[0] == line[0]:\n",
        "                        correct += 1\n",
        "                    total += 1\n",
        "    print(\"LV Accuracy =\",round(correct/total,3))\n",
        "\n",
        "def New_Leven_accuracy(fst,filename):\n",
        "    with open(filename,\"r\") as f:\n",
        "        lines = f.readlines()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for line in lines:\n",
        "            line = line.replace(\":\",\"\")\n",
        "            line = line.split()\n",
        "            for i in range(len(line) - 1):\n",
        "                f = open('new.txt', 'w+')\n",
        "                f.truncate(0)\n",
        "                f.close()\n",
        "                fileopen = open(\"new.txt\",\"w+\")\n",
        "                for j in range(len(line[i+1])):\n",
        "                    fileopen.write(\"%s \" %str(j))\n",
        "                    fileopen.write(\" %s \" %str(j+1))\n",
        "                    fileopen.write(\" %s \" %line[i+1][j])\n",
        "                    fileopen.write(\" %s \" %line[i+1][j])\n",
        "                    fileopen.write(\"0\\n\")\n",
        "                fileopen.write(str(len(line[i+1])))\n",
        "\n",
        "                fileopen.close()\n",
        "                # break\n",
        "                Lexicon_test = {}\n",
        "                Lexicon_test[line[i+1]] = line[i+1]\n",
        "                Acceptor(Lexicon_test, alphabet_indexing(alphabet), \"new.txt\")\n",
        "                !fstcompile -isymbols=vocab/chars.syms -osymbols=vocab/chars.syms new.txt > test.binfst\n",
        "                !fstcompose test.binfst fsts/New_Leven.binfst  > min_distance.binfst\n",
        "                f = open('new.txt', 'w+')\n",
        "                f.truncate(0)\n",
        "                f.close()\n",
        "                \n",
        "                !fstshortestpath min_distance.binfst | fstrmepsilon | fsttopsort | fstprint -osymbols=vocab/chars.syms | cut -f4 | grep -v \"<epsilon>\" | head -n -1 | tr -d '\\n' > new.txt\n",
        "                fileopen = open(\"new.txt\",\"r\")\n",
        "                for content in fileopen.readlines():\n",
        "                    content = content.rstrip().split(sep = '\\t')\n",
        "                    if content[0] == line[0]:\n",
        "                        correct += 1\n",
        "                    total += 1\n",
        "    print(\"EV Accuracy =\",round(correct/total,3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KH8nVh7H5elr"
      },
      "source": [
        "Leven_accuracy(_,\"spell_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2zc4vFS5fvi"
      },
      "source": [
        "New_Leven_accuracy(_,\"spell_test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ioK4YelwpW4"
      },
      "source": [
        "# Βημα 12\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_VszLpGwsj3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3cd72fe-09c3-4343-81ba-4569536354bc"
      },
      "source": [
        "import logging\n",
        "import multiprocessing\n",
        "import os\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "CORPUS = \"gutenberg\" \n",
        "raw_corpus = download_corpus(corpus=CORPUS)\n",
        "preprocessed = process_file(raw_corpus, preprocess=preprocess)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9JfZq5TowtOg"
      },
      "source": [
        "# Enable gensim logging\n",
        "logging.basicConfig(\n",
        "    format=\"%(levelname)s - %(asctime)s: %(message)s\",\n",
        "    datefmt=\"%H:%M:%S\",\n",
        "    level=logging.INFO,\n",
        ")\n",
        "\n",
        "\n",
        "class W2VLossLogger(CallbackAny2Vec):\n",
        "    \"\"\"Callback to print loss after each epoch\n",
        "    use by passing model.train(..., callbacks=[W2VLossLogger()])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.epoch = 0\n",
        "\n",
        "    def on_epoch_end(self, model):\n",
        "        loss = model.get_latest_training_loss()\n",
        "\n",
        "        if self.epoch == 0:\n",
        "            print(\"Loss after epoch {}: {}\".format(self.epoch, loss))\n",
        "        else:\n",
        "            print(\n",
        "                \"Loss after epoch {}: {}\".format(\n",
        "                    self.epoch, loss - self.loss_previous_step\n",
        "                )\n",
        "            )\n",
        "        self.epoch += 1\n",
        "        self.loss_previous_step = loss\n",
        "\n",
        "\n",
        "def train_w2v_model(\n",
        "    sentences,\n",
        "    output_file,\n",
        "    window=5,\n",
        "    embedding_dim=100,\n",
        "    epochs=300,\n",
        "    min_word_count=10,\n",
        "):\n",
        "    \"\"\"Train a word2vec model based on given sentences.\n",
        "    Args:\n",
        "        sentences list[list[str]]: List of sentences. Each element contains a list with the words\n",
        "            in the current sentence\n",
        "        output_file (str): Path to save the trained w2v model\n",
        "        window (int): w2v context size\n",
        "        embedding_dim (int): w2v vector dimension\n",
        "        epochs (int): How many epochs should the training run\n",
        "        min_word_count (int): Ignore words that appear less than min_word_count times\n",
        "    \"\"\"\n",
        "    # raise NotImplementedError(\"You should use gensim to train your w2v model\")\n",
        "    workers = multiprocessing.cpu_count()\n",
        "\n",
        "    # TODO: Instantiate gensim.models.Word2Vec class\n",
        "    model =  Word2Vec(size = embedding_dim, window = window, min_count = min_word_count)\n",
        "    # TODO: Build model vocabulary using sentences    \n",
        "    model.build_vocab(sentences, progress_per=10000)\n",
        "    # TODO: Train word2vec model\n",
        "    model.train(sentences=sentences, epochs = epochs, total_examples=model.corpus_count, total_words=model.corpus_total_words, callbacks=[W2VLossLogger()])\n",
        "    # model.train(..., callbacks=[W2VLossLogger()])\n",
        "    # Save trained model\n",
        "    model.save(output_file)\n",
        "\n",
        "    return model\n",
        "\n",
        "CORPUS = 'gutemberg'\n",
        "sentences = preprocessed\n",
        "output_file = \"gutenberg_w2v.100d.model\"\n",
        "window = 5\n",
        "embedding_dim = 100\n",
        "epochs = 1000\n",
        "min_word_count = 5\n",
        "\n",
        "train_w2v_model(\n",
        "    sentences,\n",
        "    output_file,\n",
        "    window=window,\n",
        "    embedding_dim=embedding_dim,\n",
        "    epochs=epochs,\n",
        "    min_word_count=min_word_count,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmB8HxzJ0HEO"
      },
      "source": [
        "#min to peiraxeis grapse apo panw tha sou pw ti einai ayto \n",
        "!zip -r /content/vocab.zip /content/vocab\n",
        "!zip -r /content/data.zip /content/data\n",
        "!zip -r /content/fsts.zip /content/fsts\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/vocab.zip\",\"/content/data.zip\",\"/content/fsts.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrd3qLrS2Sev"
      },
      "source": [
        "model = Word2Vec.load(\"gutenberg_w2v.100d.model\")\n",
        "prediction1 = {}\n",
        "for word in ['bible', 'book', 'bank', 'water']:\n",
        "  prediction1[word] = model.wv.most_similar(word, topn=1)\n",
        "  print(prediction1[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaJDfS4ydcis"
      },
      "source": [
        "prediction2 = {}\n",
        "for word in [('girls', 'queen', 'kings'), ('good', 'tall', 'taller'), ('france', 'paris', 'london')]:\n",
        "  prediction2[word] = model.wv.most_similar(positive=[word[0], word[2]], negative=[word[1]], topn=1)\n",
        "  print(prediction2[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqIY-ZMjiaFu"
      },
      "source": [
        "!wget  -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_frzt5QOg0iF"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit= 1000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJZM89fOhcuz"
      },
      "source": [
        "prediction1 = {}\n",
        "for word in ['bible', 'book', 'bank', 'water']:\n",
        "  prediction1[word] = model.wv.most_similar(word, topn=1)\n",
        "  print(prediction1[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaQMM7Yej9Nx"
      },
      "source": [
        "prediction2 = {}\n",
        "for word in [('girls', 'queen', 'kings'), ('good', 'tall', 'taller'), ('france', 'paris', 'london')]:\n",
        "  prediction2[word] = model.wv.most_similar(positive=[word[0], word[2]], negative=[word[1]], topn=1)\n",
        "  print(prediction2[word])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XvKmS7Fkuow"
      },
      "source": [
        "# Βημα 13"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDp89sOykxFF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07f537ca-fa3b-4299-a0c9-4aa176353b9b"
      },
      "source": [
        "!cd drive/'My Drive'/\n",
        "embedding_file = open(\"embeddings.tsv\",\"w\")\n",
        "metadata_file = open(\"metadata.tsv\",\"w\")\n",
        "model = Word2Vec.load(\"gutenberg_w2v.100d.model\")\n",
        "model.wv[\"bible\"]\n",
        "for word in lexicon:\n",
        "    for dim in range(99):\n",
        "        embedding_file.write(str(model.wv[word][dim]))\n",
        "        embedding_file.write(\"\\t\")\n",
        "    embedding_file.write(str(model.wv[word][99]))\n",
        "    embedding_file.write(\"\\n\")\n",
        "    metadata_file.write(word)\n",
        "    metadata_file.write(\"\\n\")\n",
        "embedding_file.close()\n",
        "metadata_file.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO - 18:26:00: loading Word2Vec object from gutenberg_w2v.100d.model\n",
            "INFO - 18:26:00: loading wv recursively from gutenberg_w2v.100d.model.wv.* with mmap=None\n",
            "INFO - 18:26:00: setting ignored attribute vectors_norm to None\n",
            "INFO - 18:26:00: loading vocabulary recursively from gutenberg_w2v.100d.model.vocabulary.* with mmap=None\n",
            "INFO - 18:26:00: loading trainables recursively from gutenberg_w2v.100d.model.trainables.* with mmap=None\n",
            "INFO - 18:26:00: setting ignored attribute cum_table to None\n",
            "INFO - 18:26:00: loaded gutenberg_w2v.100d.model\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkSF8knyHeRn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2457202f-701b-41a9-97ed-20da731ed459"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('embeddings.tsv') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_610e302b-9f41-424b-a10c-c2d05b8a932a\", \"embeddings.tsv\", 16228871)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkaREJ7zuT84"
      },
      "source": [
        "# Βημα 14\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2sDZ7vNuWie",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0155366-018e-40e6-8ff8-02308b7c8cb9"
      },
      "source": [
        "!wget  -c \"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-16 13:55:24--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz’\n",
            "\n",
            "aclImdb_v1.tar.gz   100%[===================>]  80.23M  22.3MB/s    in 5.3s    \n",
            "\n",
            "2020-11-16 13:55:29 (15.0 MB/s) - ‘aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vBCtjWYv7pI"
      },
      "source": [
        "!tar -xf aclImdb_v1.tar.gz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R8na9uhugsQ"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import re\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import sklearn\n",
        "\n",
        "\n",
        "data_dir = './aclImdb/'\n",
        "train_dir = os.path.join(data_dir, \"train\")\n",
        "test_dir = os.path.join(data_dir, \"test\")\n",
        "pos_train_dir = os.path.join(train_dir, \"pos\")\n",
        "neg_train_dir = os.path.join(train_dir, \"neg\")\n",
        "pos_test_dir = os.path.join(test_dir, \"pos\")\n",
        "neg_test_dir = os.path.join(test_dir, \"neg\")\n",
        "\n",
        "# For memory limitations. These parameters fit in 8GB of RAM.\n",
        "# If you have 16G of RAM you can experiment with the full dataset / W2V\n",
        "MAX_NUM_SAMPLES = 5000\n",
        "# Load first 1M word embeddings. This works because GoogleNews are roughly\n",
        "# sorted from most frequent to least frequent.\n",
        "# It may yield much worse results for other embeddings corpora\n",
        "NUM_W2V_TO_LOAD = 1000000\n",
        "\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# Fix numpy random seed for reproducibility\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "def strip_punctuation(s):\n",
        "    return re.sub(r\"[^a-zA-Z\\s]\", \" \", s)\n",
        "\n",
        "\n",
        "def preprocess(s):\n",
        "    return re.sub(\"\\s+\", \" \", strip_punctuation(s).lower())\n",
        "\n",
        "\n",
        "def tokenize(s):\n",
        "    return s.split(\" \")\n",
        "\n",
        "\n",
        "def preproc_tok(s):\n",
        "    return tokenize(preprocess(s))\n",
        "\n",
        "\n",
        "def read_samples(folder, preprocess=lambda x: x):\n",
        "    samples = glob.iglob(os.path.join(folder, \"*.txt\"))\n",
        "    data = []\n",
        "\n",
        "    for i, sample in enumerate(samples):\n",
        "        if MAX_NUM_SAMPLES > 0 and i == MAX_NUM_SAMPLES:\n",
        "            break\n",
        "        with open(sample, \"r\") as fd:\n",
        "            x = [preprocess(l) for l in fd][0]\n",
        "            data.append(x)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def create_corpus(pos, neg):\n",
        "    corpus = np.array(pos + neg)\n",
        "    y = np.array([1 for _ in pos] + [0 for _ in neg])\n",
        "    indices = np.arange(y.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "\n",
        "    return list(corpus[indices]), list(y[indices])\n",
        "\n",
        "\n",
        "def extract_nbow(corpus,model,dimentions):\n",
        "    \"\"\"Extract neural bag of words representations\"\"\"\n",
        "    nbow = np.zeros((len(corpus),dimentions))\n",
        "    for i in range(len(corpus)):\n",
        "        for j in range(len(corpus[i])):\n",
        "            if corpus[i][j] in model.wv.vocab:\n",
        "                nbow[i] +=  model.wv[corpus[i][j]]\n",
        "        nbow[i] = nbow[i]/ len(corpus[i])\n",
        "    return nbow\n",
        "    # raise NotImplementedError(\"Implement nbow extractor\")\n",
        "\n",
        "\n",
        "def train_sentiment_analysis(train_corpus, train_labels):\n",
        "    \"\"\"Train a sentiment analysis classifier using NBOW + Logistic regression\"\"\"\n",
        "    logisticRegr = LogisticRegression()\n",
        "    logisticRegr.fit(train_corpus, train_labels)\n",
        "    return logisticRegr\n",
        "    # raise NotImplementedError(\"Implement sentiment analysis training\")\n",
        "\n",
        "\n",
        "def evaluate_sentiment_analysis(classifier, test_corpus, test_labels):\n",
        "    \"\"\"Evaluate classifier in the test corpus and report accuracy\"\"\"\n",
        "    y_pred = classifier.predict(test_corpus)\n",
        "    correct = 0\n",
        "    for i in range(len(y_pred)):\n",
        "        if int(y_pred[i]) == test_labels[i]:\n",
        "            correct += 1\n",
        "    return round(correct/len(y_pred),3)\n",
        "\n",
        "\n",
        "positive_train_data = read_samples(pos_train_dir)\n",
        "negative_train_data = read_samples(neg_train_dir)\n",
        "positive_test_data = read_samples(pos_test_dir)\n",
        "negative_test_data = read_samples(neg_test_dir)\n",
        "whole_positive = positive_train_data + positive_test_data\n",
        "whole_negative = negative_train_data + negative_test_data\n",
        "\n",
        "train_corpus,train_labels = create_corpus(whole_positive[0:8000],whole_negative[0:8000])\n",
        "test_corpus,test_labels = create_corpus(whole_positive[8000:9999],whole_negative[8000:9999])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRByFfE8xBbN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7b06d5c-923d-4123-9708-d24c61bb2ae7"
      },
      "source": [
        "model = Word2Vec.load(\"gutenberg_w2v.100d.model\")\n",
        "train_nbow = extract_nbow(train_corpus,model,100)\n",
        "test_nbow = extract_nbow(test_corpus,model,100)\n",
        "Reg = train_sentiment_analysis(train_nbow,train_labels)\n",
        "evaluate_sentiment_analysis(Reg,test_nbow,test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Our model had accuracy = 0.612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMDy-T0gYjPz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "553d1cce-556d-4bfd-ca39-5b366632faa6"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True, limit= 1000000)\n",
        "train_nbow = extract_nbow(train_corpus,model,300)\n",
        "test_nbow = extract_nbow(test_corpus,model,300)\n",
        "Reg = train_sentiment_analysis(train_nbow,train_labels)\n",
        "evaluate_sentiment_analysis(Reg,test_nbow,test_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Google's model had accuracy = 0.602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZtR3OoNzY8H"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}